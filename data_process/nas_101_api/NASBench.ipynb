{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SRxqMakh3PRY"
      },
      "source": [
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "47ieDn-jNLYd"
      },
      "source": [
        "# NASBench-101\n",
        "\n",
        "This colab accompanies [**NAS-Bench-101: Towards Reproducible Neural Architecture Search**](https://arxiv.org/abs/1902.09635) and the rest of the code at https://github.com/google-research/nasbench.\n",
        "\n",
        "In this colab, we demonstrate how to use the dataset for simple benchmarking and analysis. The publicly available and free hosted colab instances are sufficient to run this colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lBNMsBUS3SAq"
      },
      "source": [
        "## Load NASBench library and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "colab_type": "code",
        "id": "vl1oLYux3FhJ",
        "outputId": "260484df-3a86-48ef-88a4-f8e441d524bc"
      },
      "outputs": [],
      "source": [
        "# This code was written in TF 1.12 but should be supported all the way through\n",
        "# TF 1.15. Untested in TF 2.0+.\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "# Download the raw data (only 108 epoch data points, for full dataset,\n",
        "# uncomment the second line for nasbench_full.tfrecord).\n",
        "\n",
        "!curl -O https://storage.googleapis.com/nasbench/nasbench_only108.tfrecord\n",
        "# !curl -O https://storage.googleapis.com/nasbench/nasbench_full.tfrecord\n",
        "\n",
        "# Clone and install the code and dependencies.\n",
        "\n",
        "!git clone https://github.com/google-research/nasbench\n",
        "!pip install ./nasbench\n",
        "\n",
        "# Initialize the NASBench object which parses the raw data into memory (this\n",
        "# should only be run once as it takes up to a few minutes).\n",
        "from nasbench import api\n",
        "\n",
        "# Use nasbench_full.tfrecord for full dataset (run download command above).\n",
        "nasbench = api.NASBench('nasbench_only108.tfrecord')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the NASBench object which parses the raw data into memory (this\n",
        "# should only be run once as it takes up to a few minutes).\n",
        "import tensorflow as tf\n",
        "import sys \n",
        "# 切到 TF1 兼容模式\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "# 关键：让后续模块看到的 \"import tensorflow as tf\" 就是 TF1 风格\n",
        "sys.modules['tensorflow'] = tf.compat.v1\n",
        "\n",
        "# 若你下面还要用变量名 tf，建议也指向 v1\n",
        "tf = tf.compat.v1\n",
        "\n",
        "from nasbench import api\n",
        "\n",
        "# Use nasbench_full.tfrecord for full dataset (run download command above).\n",
        "nasbench = api.NASBench('nasbench_full.tfrecord')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "oFhFRmck7NzM"
      },
      "outputs": [],
      "source": [
        "# Standard imports\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Useful constants\n",
        "INPUT = 'input'\n",
        "OUTPUT = 'output'\n",
        "CONV3X3 = 'conv3x3-bn-relu'\n",
        "CONV1X1 = 'conv1x1-bn-relu'\n",
        "MAXPOOL3X3 = 'maxpool3x3'\n",
        "NUM_VERTICES = 7\n",
        "MAX_EDGES = 9\n",
        "EDGE_SPOTS = NUM_VERTICES * (NUM_VERTICES - 1) / 2   # Upper triangular matrix\n",
        "OP_SPOTS = NUM_VERTICES - 2   # Input/output vertices are fixed\n",
        "ALLOWED_OPS = [CONV3X3, CONV1X1, MAXPOOL3X3]\n",
        "ALLOWED_EDGES = [0, 1]   # Binary adjacency matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "llC2AebQOWq9"
      },
      "source": [
        "## Basic usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "colab_type": "code",
        "id": "kZvm6i0VGP_M",
        "outputId": "2999b6a6-9f1d-4361-8be9-ee54afdb82be"
      },
      "outputs": [],
      "source": [
        "# Query an Inception-like cell from the dataset.\n",
        "cell = api.ModelSpec(\n",
        "  matrix=[[0, 1, 1, 1, 0, 1, 0],    # input layer\n",
        "          [0, 0, 0, 0, 0, 0, 1],    # 1x1 conv\n",
        "          [0, 0, 0, 0, 0, 0, 1],    # 3x3 conv\n",
        "          [0, 0, 0, 0, 1, 0, 0],    # 5x5 conv (replaced by two 3x3's)\n",
        "          [0, 0, 0, 0, 0, 0, 1],    # 5x5 conv (replaced by two 3x3's)\n",
        "          [0, 0, 0, 0, 0, 0, 1],    # 3x3 max-pool\n",
        "          [0, 0, 0, 0, 0, 0, 0]],   # output layer\n",
        "  # Operations at the vertices of the module, matches order of matrix.\n",
        "  ops=[INPUT, CONV1X1, CONV3X3, CONV3X3, CONV3X3, MAXPOOL3X3, OUTPUT])\n",
        "\n",
        "# Querying multiple times may yield different results. Each cell is evaluated 3\n",
        "# times at each epoch budget and querying will sample one randomly.\n",
        "data = nasbench.query(cell)\n",
        "for k, v in data.items():\n",
        "  print('%s: %s' % (k, str(v)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-22 14:59:24.467411: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-08-22 14:59:24.493588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-08-22 14:59:24.883859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/rtx4090/anaconda3/lib/python3.12/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Loading dataset from file... This may take a few minutes...\n",
            "WARNING:tensorflow:From /home/rtx4090/code/python/current/LightNAS/data_process/nas_101_api/nasbench/api.py:149: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "Loaded dataset in 43 seconds\n"
          ]
        }
      ],
      "source": [
        "# Initialize the NASBench object which parses the raw data into memory (this\n",
        "# should only be run once as it takes up to a few minutes).\n",
        "import tensorflow as tf\n",
        "import sys \n",
        "# 切到 TF1 兼容模式\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "# 关键：让后续模块看到的 \"import tensorflow as tf\" 就是 TF1 风格\n",
        "sys.modules['tensorflow'] = tf.compat.v1\n",
        "\n",
        "# 若你下面还要用变量名 tf，建议也指向 v1\n",
        "tf = tf.compat.v1\n",
        "\n",
        "from nasbench import api\n",
        "\n",
        "# Use nasbench_full.tfrecord for full dataset (run download command above).\n",
        "nasbench = api.NASBench('nasbench_full.tfrecord')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型哈希值（key）: 00005c142e6f48ac74fdcf73e3439874\n",
            "邻接矩阵: [[0 1 0 0 1 1 0]\n",
            " [0 0 1 0 0 0 0]\n",
            " [0 0 0 1 0 0 1]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0]]\n",
            "操作序列: ['input', 'conv3x3-bn-relu', 'maxpool3x3', 'conv3x3-bn-relu', 'conv3x3-bn-relu', 'conv1x1-bn-relu', 'output']\n",
            "可训练参数数量: 8555530\n",
            "训练108轮，第1次重复:\n",
            "最终测试准确率: 0.9211738705635071\n",
            "训练时间: 1769.1279296875\n",
            "训练108轮，第2次重复:\n",
            "最终测试准确率: 0.9190705418586731\n",
            "训练时间: 1768.2509765625\n",
            "训练108轮，第3次重复:\n",
            "最终测试准确率: 0.9215745329856873\n",
            "训练时间: 1768.9759521484375\n",
            "训练4轮，第1次重复:\n",
            "最终测试准确率: 0.3504607379436493\n",
            "训练时间: 89.90399932861328\n",
            "训练4轮，第2次重复:\n",
            "最终测试准确率: 0.31870993971824646\n",
            "训练时间: 90.87100219726562\n",
            "训练4轮，第3次重复:\n",
            "最终测试准确率: 0.3530648946762085\n",
            "训练时间: 89.80400085449219\n",
            "训练12轮，第1次重复:\n",
            "最终测试准确率: 0.6185897588729858\n",
            "训练时间: 216.27999877929688\n",
            "训练12轮，第2次重复:\n",
            "最终测试准确率: 0.4196714758872986\n",
            "训练时间: 215.625\n",
            "训练12轮，第3次重复:\n",
            "最终测试准确率: 0.42327722907066345\n",
            "训练时间: 220.11300659179688\n",
            "训练36轮，第1次重复:\n",
            "最终测试准确率: 0.8817107081413269\n",
            "训练时间: 613.8470458984375\n",
            "训练36轮，第2次重复:\n",
            "最终测试准确率: 0.8892227411270142\n",
            "训练时间: 614.22705078125\n",
            "训练36轮，第3次重复:\n",
            "最终测试准确率: 0.8898237347602844\n",
            "训练时间: 613.9719848632812\n"
          ]
        }
      ],
      "source": [
        "# 2. 遍历数据集中的唯一模型哈希值（也可直接使用已知的哈希值作为 key）\n",
        "for unique_hash in nasbench.hash_iterator():\n",
        "    # 3. 通过哈希值（key）查询模型信息\n",
        "    fixed_metrics, computed_metrics = nasbench.get_metrics_from_hash(unique_hash)\n",
        "    \n",
        "    # 4. 输出模型的固定统计信息（如结构信息）\n",
        "    print(\"模型哈希值（key）:\", unique_hash)\n",
        "    print(\"邻接矩阵:\", fixed_metrics['module_adjacency'])\n",
        "    print(\"操作序列:\", fixed_metrics['module_operations'])\n",
        "    print(\"可训练参数数量:\", fixed_metrics['trainable_parameters'])\n",
        "    \n",
        "    # 5. 输出模型的计算统计信息（如不同轮次的性能）\n",
        "    for epochs in nasbench.valid_epochs:\n",
        "        for repeat_index in range(len(computed_metrics[epochs])):\n",
        "            data_point = computed_metrics[epochs][repeat_index]\n",
        "            print(f\"训练{epochs}轮，第{repeat_index + 1}次重复:\")\n",
        "            print(\"最终测试准确率:\", data_point['final_test_accuracy'])\n",
        "            print(\"训练时间:\", data_point['final_training_time'])\n",
        "    \n",
        "    # 为避免输出过多，此处仅遍历第一个模型后退出\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import nasbench.api as api\n",
        "# # 1) 加载 NASBench\n",
        "# nasbench = api.NASBench('nasbench_full.tfrecord')\n",
        "# print(f\"✅ 数据集加载完成，可用评估 epoch：{nasbench.valid_epochs}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "423624it [00:21, 19955.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ 已保存到 ../101_acc_data.pkl，共 6 个条目\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from tqdm import *\n",
        "\n",
        "OP_TYPES = ['input','conv1x1-bn-relu','conv3x3-bn-relu','maxpool3x3','output']\n",
        "\n",
        "def compute_flops(matrix, ops, input_channels=3, output_channels=16):\n",
        "    flops = 0\n",
        "    num_vertices = len(ops)\n",
        "    channels = [input_channels] + [output_channels]*(num_vertices-2) + [output_channels]\n",
        "    H = W = 32\n",
        "    for i in range(num_vertices):\n",
        "        for j in range(num_vertices):\n",
        "            if matrix[i][j] == 1 and ops[i] != 'input' and ops[j] != 'output':\n",
        "                op = ops[i]\n",
        "                if op == 'conv1x1-bn-relu':\n",
        "                    flops += (1*1*channels[i]) * channels[j] * (H*W)\n",
        "                elif op == 'conv3x3-bn-relu':\n",
        "                    flops += (3*3*channels[i]) * channels[j] * (H*W)\n",
        "    return int(flops)\n",
        "\n",
        "def op_to_onehot(op, op_list):\n",
        "    onehot = [0]*len(op_list)\n",
        "    if op in op_list:\n",
        "        onehot[op_list.index(op)] = 1\n",
        "    return onehot\n",
        "\n",
        "\n",
        "# 目标 epoch：108\n",
        "target_epochs = 108\n",
        "if target_epochs not in nasbench.valid_epochs:\n",
        "    raise ValueError(f\"目标 epoch={target_epochs} 不在可用列表 {nasbench.valid_epochs} 中\")\n",
        "\n",
        "results = {}\n",
        "data = {\n",
        "    \"key\": [],\n",
        "    \"adj_matrix\": [],\n",
        "    \"features\": [],\n",
        "    \"flops\": [],\n",
        "    \"params\": [],\n",
        "    \"accuracy\": [],\n",
        "    # \"latency\": [],\n",
        "}\n",
        "for idx, unique_hash in tqdm(enumerate(nasbench.hash_iterator())):\n",
        "    fixed_metrics, computed_metrics = nasbench.get_metrics_from_hash(unique_hash)\n",
        "\n",
        "    matrix = fixed_metrics['module_adjacency'].tolist()\n",
        "    ops = fixed_metrics['module_operations']\n",
        "    params = int(fixed_metrics['trainable_parameters'])\n",
        "    flops = compute_flops(np.array(matrix), ops)\n",
        "    node_ids = [OP_TYPES.index(op) for op in ops]\n",
        "\n",
        "    mean_metrics = {}\n",
        "    if target_epochs in computed_metrics:\n",
        "        repeats = computed_metrics[target_epochs]  # list[dict]\n",
        "        if repeats:\n",
        "            values = [float(rep.get('final_test_accuracy', 0.0)) for rep in repeats]\n",
        "            mean_metrics['final_test_accuracy'] = float(np.mean(values))\n",
        "    accuracy = mean_metrics['final_test_accuracy']\n",
        "    \n",
        "    results[idx] = {\n",
        "        'adj_matrix': matrix,\n",
        "        'node_operations': ops,\n",
        "        'features': node_ids,\n",
        "        'flops': flops,\n",
        "        'params': params,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    \n",
        "    data[\"key\"].append(idx)\n",
        "    data[\"adj_matrix\"].append(matrix)\n",
        "    data[\"features\"].append(node_ids)\n",
        "    data[\"flops\"].append(flops)\n",
        "    data[\"params\"].append(params)\n",
        "    data[\"accuracy\"].append(accuracy)\n",
        "    \n",
        "\n",
        "out_path = f'../101_acc_data.pkl'\n",
        "\n",
        "with open(out_path, 'wb') as f:\n",
        "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\"✨ 已保存到 {out_path}，共 {len(data)} 个条目\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (7, 7), count: 359082\n",
            "shape: (6, 6), count: 62010\n",
            "shape: (4, 4), count: 84\n",
            "shape: (5, 5), count: 2441\n",
            "shape: (2, 2), count: 1\n",
            "shape: (3, 3), count: 6\n",
            "一共 6 种不同的 shape\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "graphs = data['adj_matrix']\n",
        "shapes = [np.array(g).shape for g in graphs]   # 每个图的形状\n",
        "shape_counts = Counter(shapes)\n",
        "\n",
        "for s, c in shape_counts.items():\n",
        "    print(f\"shape: {s}, count: {c}\")\n",
        "\n",
        "print(f\"一共 {len(shape_counts)} 种不同的 shape\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shape: (7,), count: 359082\n",
            "shape: (6,), count: 62010\n",
            "shape: (4,), count: 84\n",
            "shape: (5,), count: 2441\n",
            "shape: (2,), count: 1\n",
            "shape: (3,), count: 6\n",
            "一共 6 种不同的 shape\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "graphs = data['features']\n",
        "shapes = [np.array(g).shape for g in graphs]   # 每个图的形状\n",
        "shape_counts = Counter(shapes)\n",
        "\n",
        "for s, c in shape_counts.items():\n",
        "    print(f\"shape: {s}, count: {c}\")\n",
        "\n",
        "print(f\"一共 {len(shape_counts)} 种不同的 shape\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✨ 已保存到 ../101_acc_data.pkl，共 423624 个条目\n"
          ]
        }
      ],
      "source": [
        "out_path = f'../101_acc_data.pkl'\n",
        "with open(out_path, 'wb') as f:\n",
        "    pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "print(f\"✨ 已保存到 {out_path}，共 {len(data['accuracy'])} 个条目\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../101_acc_data.pkl'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m out_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../101_acc_data.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(out_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     df \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f, protocol\u001b[38;5;241m=\u001b[39mpickle\u001b[38;5;241m.\u001b[39mHIGHEST_PROTOCOL)\n\u001b[1;32m      4\u001b[0m df\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../101_acc_data.pkl'"
          ]
        }
      ],
      "source": [
        "out_path = f'../101_acc_data.pkl'\n",
        "with open(out_path, 'rb') as f:\n",
        "    df = pickle.load(f)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uXnVdG32Oe19"
      },
      "source": [
        "## Example search experiment (random vs. evolution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Xtl_Aqr7OeOF"
      },
      "outputs": [],
      "source": [
        "def random_spec():\n",
        "  \"\"\"Returns a random valid spec.\"\"\"\n",
        "  while True:\n",
        "    matrix = np.random.choice(ALLOWED_EDGES, size=(NUM_VERTICES, NUM_VERTICES))\n",
        "    matrix = np.triu(matrix, 1)\n",
        "    ops = np.random.choice(ALLOWED_OPS, size=(NUM_VERTICES)).tolist()\n",
        "    ops[0] = INPUT\n",
        "    ops[-1] = OUTPUT\n",
        "    spec = api.ModelSpec(matrix=matrix, ops=ops)\n",
        "    if nasbench.is_valid(spec):\n",
        "      return spec\n",
        "\n",
        "def mutate_spec(old_spec, mutation_rate=1.0):\n",
        "  \"\"\"Computes a valid mutated spec from the old_spec.\"\"\"\n",
        "  while True:\n",
        "    new_matrix = copy.deepcopy(old_spec.original_matrix)\n",
        "    new_ops = copy.deepcopy(old_spec.original_ops)\n",
        "\n",
        "    # In expectation, V edges flipped (note that most end up being pruned).\n",
        "    edge_mutation_prob = mutation_rate / NUM_VERTICES\n",
        "    for src in range(0, NUM_VERTICES - 1):\n",
        "      for dst in range(src + 1, NUM_VERTICES):\n",
        "        if random.random() < edge_mutation_prob:\n",
        "          new_matrix[src, dst] = 1 - new_matrix[src, dst]\n",
        "          \n",
        "    # In expectation, one op is resampled.\n",
        "    op_mutation_prob = mutation_rate / OP_SPOTS\n",
        "    for ind in range(1, NUM_VERTICES - 1):\n",
        "      if random.random() < op_mutation_prob:\n",
        "        available = [o for o in nasbench.config['available_ops'] if o != new_ops[ind]]\n",
        "        new_ops[ind] = random.choice(available)\n",
        "        \n",
        "    new_spec = api.ModelSpec(new_matrix, new_ops)\n",
        "    if nasbench.is_valid(new_spec):\n",
        "      return new_spec\n",
        "\n",
        "def random_combination(iterable, sample_size):\n",
        "  \"\"\"Random selection from itertools.combinations(iterable, r).\"\"\"\n",
        "  pool = tuple(iterable)\n",
        "  n = len(pool)\n",
        "  indices = sorted(random.sample(range(n), sample_size))\n",
        "  return tuple(pool[i] for i in indices)\n",
        "\n",
        "def run_random_search(max_time_budget=5e6):\n",
        "  \"\"\"Run a single roll-out of random search to a fixed time budget.\"\"\"\n",
        "  nasbench.reset_budget_counters()\n",
        "  times, best_valids, best_tests = [0.0], [0.0], [0.0]\n",
        "  while True:\n",
        "    spec = random_spec()\n",
        "    data = nasbench.query(spec)\n",
        "\n",
        "    # It's important to select models only based on validation accuracy, test\n",
        "    # accuracy is used only for comparing different search trajectories.\n",
        "    if data['validation_accuracy'] > best_valids[-1]:\n",
        "      best_valids.append(data['validation_accuracy'])\n",
        "      best_tests.append(data['test_accuracy'])\n",
        "    else:\n",
        "      best_valids.append(best_valids[-1])\n",
        "      best_tests.append(best_tests[-1])\n",
        "\n",
        "    time_spent, _ = nasbench.get_budget_counters()\n",
        "    times.append(time_spent)\n",
        "    if time_spent > max_time_budget:\n",
        "      # Break the first time we exceed the budget.\n",
        "      break\n",
        "\n",
        "  return times, best_valids, best_tests\n",
        "\n",
        "def run_evolution_search(max_time_budget=5e6,\n",
        "                         population_size=50,\n",
        "                         tournament_size=10,\n",
        "                         mutation_rate=1.0):\n",
        "  \"\"\"Run a single roll-out of regularized evolution to a fixed time budget.\"\"\"\n",
        "  nasbench.reset_budget_counters()\n",
        "  times, best_valids, best_tests = [0.0], [0.0], [0.0]\n",
        "  population = []   # (validation, spec) tuples\n",
        "\n",
        "  # For the first population_size individuals, seed the population with randomly\n",
        "  # generated cells.\n",
        "  for _ in range(population_size):\n",
        "    spec = random_spec()\n",
        "    data = nasbench.query(spec)\n",
        "    time_spent, _ = nasbench.get_budget_counters()\n",
        "    times.append(time_spent)\n",
        "    population.append((data['validation_accuracy'], spec))\n",
        "\n",
        "    if data['validation_accuracy'] > best_valids[-1]:\n",
        "      best_valids.append(data['validation_accuracy'])\n",
        "      best_tests.append(data['test_accuracy'])\n",
        "    else:\n",
        "      best_valids.append(best_valids[-1])\n",
        "      best_tests.append(best_tests[-1])\n",
        "\n",
        "    if time_spent > max_time_budget:\n",
        "      break\n",
        "\n",
        "  # After the population is seeded, proceed with evolving the population.\n",
        "  while True:\n",
        "    sample = random_combination(population, tournament_size)\n",
        "    best_spec = sorted(sample, key=lambda i:i[0])[-1][1]\n",
        "    new_spec = mutate_spec(best_spec, mutation_rate)\n",
        "\n",
        "    data = nasbench.query(new_spec)\n",
        "    time_spent, _ = nasbench.get_budget_counters()\n",
        "    times.append(time_spent)\n",
        "\n",
        "    # In regularized evolution, we kill the oldest individual in the population.\n",
        "    population.append((data['validation_accuracy'], new_spec))\n",
        "    population.pop(0)\n",
        "\n",
        "    if data['validation_accuracy'] > best_valids[-1]:\n",
        "      best_valids.append(data['validation_accuracy'])\n",
        "      best_tests.append(data['test_accuracy'])\n",
        "    else:\n",
        "      best_valids.append(best_valids[-1])\n",
        "      best_tests.append(best_tests[-1])\n",
        "\n",
        "    if time_spent > max_time_budget:\n",
        "      break\n",
        "\n",
        "  return times, best_valids, best_tests\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "colab_type": "code",
        "id": "HMfF2zXxpQNA",
        "outputId": "edff2ca4-8a55-4537-838a-dedc7a9f360b"
      },
      "outputs": [],
      "source": [
        "# Run random search and evolution search 10 times each. This should take a few\n",
        "# minutes to run. Note that each run would have taken days of compute to\n",
        "# actually train and evaluate if the dataset were not precomputed.\n",
        "random_data = []\n",
        "evolution_data = []\n",
        "for repeat in range(10):\n",
        "  print('Running repeat %d' % (repeat + 1))\n",
        "  times, best_valid, best_test = run_random_search()\n",
        "  random_data.append((times, best_valid, best_test))\n",
        "\n",
        "  times, best_valid, best_test = run_evolution_search()\n",
        "  evolution_data.append((times, best_valid, best_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "colab_type": "code",
        "id": "2d-yRmuhkz35",
        "outputId": "2ae12897-074f-4705-db94-6be1aa4208dc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "for times, best_valid, best_test in random_data:\n",
        "  plt.plot(times, best_valid, label='valid', color='red', alpha=0.5)\n",
        "  plt.plot(times, best_test, label='test', color='blue', alpha=0.5)\n",
        "\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('time spent (seconds)')\n",
        "plt.ylim(0.92, 0.96)\n",
        "plt.grid()\n",
        "plt.title('Random search trajectories (red=validation, blue=test)')\n",
        "\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "for times, best_valid, best_test in evolution_data:\n",
        "  plt.plot(times, best_valid, label='valid', color='red', alpha=0.5)\n",
        "  plt.plot(times, best_test, label='test', color='blue', alpha=0.5)\n",
        "\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('time spent (seconds)')\n",
        "plt.ylim(0.92, 0.96)\n",
        "plt.grid()\n",
        "plt.title('Evolution search trajectories (red=validation, blue=test)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "colab_type": "code",
        "id": "F9lB_qL2oz9M",
        "outputId": "2a9d25c8-3adb-456a-ae2d-3229a7ed8bfe"
      },
      "outputs": [],
      "source": [
        "# Compare the mean test accuracy along with error bars.\n",
        "def plot_data(data, color, label, gran=10000, max_budget=5000000):\n",
        "  \"\"\"Computes the mean and IQR fixed time steps.\"\"\"\n",
        "  xs = range(0, max_budget+1, gran)\n",
        "  mean = [0.0]\n",
        "  per25 = [0.0]\n",
        "  per75 = [0.0]\n",
        "  \n",
        "  repeats = len(data)\n",
        "  pointers = [1 for _ in range(repeats)]\n",
        "  \n",
        "  cur = gran\n",
        "  while cur < max_budget+1:\n",
        "    all_vals = []\n",
        "    for repeat in range(repeats):\n",
        "      while (pointers[repeat] < len(data[repeat][0]) and \n",
        "             data[repeat][0][pointers[repeat]] < cur):\n",
        "        pointers[repeat] += 1\n",
        "      prev_time = data[repeat][0][pointers[repeat]-1]\n",
        "      prev_test = data[repeat][2][pointers[repeat]-1]\n",
        "      next_time = data[repeat][0][pointers[repeat]]\n",
        "      next_test = data[repeat][2][pointers[repeat]]\n",
        "      assert prev_time < cur and next_time >= cur\n",
        "\n",
        "      # Linearly interpolate the test between the two surrounding points\n",
        "      cur_val = ((cur - prev_time) / (next_time - prev_time)) * (next_test - prev_test) + prev_test\n",
        "      \n",
        "      all_vals.append(cur_val)\n",
        "      \n",
        "    all_vals = sorted(all_vals)\n",
        "    mean.append(sum(all_vals) / float(len(all_vals)))\n",
        "    per25.append(all_vals[int(0.25 * repeats)])\n",
        "    per75.append(all_vals[int(0.75 * repeats)])\n",
        "      \n",
        "    cur += gran\n",
        "    \n",
        "  plt.plot(xs, mean, color=color, label=label, linewidth=2)\n",
        "  plt.fill_between(xs, per25, per75, alpha=0.1, linewidth=0, facecolor=color)\n",
        "\n",
        "plot_data(random_data, 'red', 'random')\n",
        "plot_data(evolution_data, 'blue', 'evolution')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylim(0.92, 0.95)\n",
        "plt.xlabel('total training time spent (seconds)')\n",
        "plt.ylabel('accuracy')\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DPnRtp0zXUDh"
      },
      "source": [
        "## More information\n",
        "\n",
        "For more information on using the dataset, see the API documentation at https://github.com/google-research/nasbench/blob/master/nasbench/api.py.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NASBench.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
