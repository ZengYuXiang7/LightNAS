{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00a77ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'module.name' from '/home/rtx4090/code/python/current/LightNAS/configs/OurModelConfig.py'> OurModelConfig\n",
      "✅ All __pycache__ folders removed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OurModelConfig(classification=False, ablation=0, try_exp=1, ts_var=1, input_size=192, bs=256, lr=0.001, decay=0.0001, loss_func='MSELoss', optim='Adam', epochs=2000, patience=200, verbose=2000, device='cuda', use_amp=False, monitor_metric='KendallTau', monitor_reverse=True, path='./data', task='bench201', dataset='101_acc', predict_target='accuracy', eval_set=True, shuffle=False, scaler_method='minmax', spliter_ratio='5:4:91', sample_method='random', logger='zyx', model='ours', d_model=192, num_layers=4, retrain=True, seed=0, rounds=5, runid=0, debug=False, record=True, hyper_search=False, continue_train=False, op_encoder='embedding', lp_d_model=8, lap_node_id_sign_flip=True, num_heads=4, att_method='self', att_bias=True, transfer=False, rank_loss=True, ac_loss=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experiment Settings, logger, plotter\n",
    "from utils.exp_logger import Logger\n",
    "from utils.exp_metrics_plotter import MetricsPlotter\n",
    "from utils.utils import set_settings\n",
    "from utils.exp_config import get_config\n",
    "\n",
    "config = get_config(\"OurModelConfig\")\n",
    "set_settings(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e89ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = './data/nnlqp/unseen_structure/'\n",
    "all_latency_file = data_root + \"/gt.txt\"\n",
    "test_model_type = ['resnet18' 'vgg16' 'efficientb0' 'mobilenetv2' 'mobilenetv3' 'mnasnet' 'alexnet' 'squeezenet' 'googlenet' 'nasbench201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9317659",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(all_latency_file)\u001b[38;5;241m.\u001b[39mreadlines():\n\u001b[1;32m      3\u001b[0m     model_types\u001b[38;5;241m.\u001b[39madd(line\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m4\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_model_type \u001b[38;5;129;01min\u001b[39;00m model_types\n\u001b[1;32m      5\u001b[0m test_model_types \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([test_model_type])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train_model_types:\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "model_types = set()\n",
    "for line in open(all_latency_file).readlines():\n",
    "    model_types.add(line.split()[4])\n",
    "assert test_model_type in model_types\n",
    "test_model_types = set([test_model_type])\n",
    "if train_model_types:\n",
    "    train_model_types = set(train_model_types.split(','))\n",
    "    train_model_types = train_model_types & model_types\n",
    "else:\n",
    "    train_model_types = model_types - test_model_types\n",
    "assert len(train_model_types) > 0\n",
    "allow_platforms=multi_plt.keys() if multi_plt else None\n",
    "\n",
    "self.logger.info(\"Train model types: {}\".format(train_model_types))\n",
    "self.logger.info(\"Test model types: {}\".format(test_model_types))\n",
    "self.logger.info(\"Platforms: {}\".format(allow_platforms))\n",
    "\n",
    "train_set = GraphLatencyDataset(\n",
    "    data_root,\n",
    "    onnx_dir,\n",
    "    all_latency_file,\n",
    "    override_data=override_data,\n",
    "    model_types=train_model_types,\n",
    "    platforms=allow_platforms,\n",
    ")\n",
    "test_set = GraphLatencyDataset(\n",
    "    data_root,\n",
    "    onnx_dir,\n",
    "    all_latency_file,\n",
    "    override_data=override_data,\n",
    "    model_types=test_model_types,\n",
    "    platforms=allow_platforms,\n",
    ")\n",
    "\n",
    "self.logger.info(\"Train data = {}, Test data = {}\".format(len(train_set), len(test_set)))\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dfb06d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OurModelConfig' object has no attribute 'data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m    俩个场景\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m    1、Out of domain latency prediction on NNLQ\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    2、In domain latency prediction on NNLQ\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m trainset, testset, trtypes, tetypes \u001b[38;5;241m=\u001b[39m init_dataset_NNLQP(\n\u001b[0;32m----> 7\u001b[0m     config\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m      8\u001b[0m     config\u001b[38;5;241m.\u001b[39mtest_model_type,\n\u001b[1;32m      9\u001b[0m     config\u001b[38;5;241m.\u001b[39moverride_data,\n\u001b[1;32m     10\u001b[0m     config\u001b[38;5;241m.\u001b[39membed_type,\n\u001b[1;32m     11\u001b[0m     config\u001b[38;5;241m.\u001b[39mfinetuning,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     13\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain model types: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(trtypes))\n\u001b[1;32m     14\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest model types: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(tetypes))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'OurModelConfig' object has no attribute 'data_path'"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "    俩个场景\n",
    "    1、Out of domain latency prediction on NNLQ\n",
    "    2、In domain latency prediction on NNLQ\n",
    "\"\"\"\n",
    "trainset, testset, trtypes, tetypes = init_dataset_NNLQP(\n",
    "    config.data_path,\n",
    "    config.test_model_type,\n",
    "    config.override_data,\n",
    "    config.embed_type,\n",
    "    config.finetuning,\n",
    ")\n",
    "print(\"Train model types: {}\".format(trtypes))\n",
    "print(\"Test model types: {}\".format(tetypes))\n",
    "\n",
    "train_sampler = FixedLengthBatchSampler(\n",
    "    trainset, config.dataset, config.batch_size, include_partial=True\n",
    ")\n",
    "test_sampler = FixedLengthBatchSampler(\n",
    "    testset, config.dataset, config.batch_size, include_partial=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=config.n_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    batch_sampler=train_sampler,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    testset,\n",
    "    shuffle=(test_sampler is None),\n",
    "    num_workers=config.n_workers,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    batch_sampler=test_sampler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50151b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "    mobile 角度？\n",
    "    cpu -> gpu tpu dsp\n",
    "    gpu -> cpu tpu dsp\n",
    "    tpu -> cpu gpu dsp\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_bench201_latency(config):\n",
    "    with open(\"./data/nasbench201/pkl/desktop-cpu-core-i7-7820x-fp32.pkl\", \"rb\") as f:\n",
    "        df = pickle.load(f)\n",
    "    api = API(\"./data_process/nas_201_api/NAS-Bench-201-v1_0-e61699.pth\", verbose=False)\n",
    "    import numpy as np\n",
    "\n",
    "    data = {\n",
    "        \"key\": [],\n",
    "        \"adj_matrix\": [],\n",
    "        \"features\": [],\n",
    "        \"flops\": [],\n",
    "        \"params\": [],\n",
    "        \"accuracy\": [],\n",
    "        \"latency\": [],\n",
    "    }\n",
    "\n",
    "    for i in trange(len(df)):\n",
    "        try:\n",
    "            key = np.asarray(df[i, :-1], dtype=np.int32)\n",
    "            arch_str = get_arch_str_from_arch_vector(key)\n",
    "            index = api.query_index_by_arch(arch_str)\n",
    "\n",
    "            cost_info = api.get_cost_info(index, dataset=\"cifar10-valid\")\n",
    "            flops = float(cost_info[\"flops\"])\n",
    "            params = float(cost_info[\"params\"])\n",
    "\n",
    "            adj_matrix, label = get_matrix_and_ops(key)\n",
    "            adj_matrix, features = get_adjacency_and_features(adj_matrix, label)\n",
    "            features = np.argmax(features, axis=1)\n",
    "\n",
    "            acc_info = api.get_more_info(\n",
    "                index, dataset=\"cifar10-valid\", iepoch=None, hp=\"200\", is_random=False\n",
    "            )\n",
    "\n",
    "            accuracy = float(acc_info[\"test-accuracy\"])\n",
    "            latency = float(df[i, -1])\n",
    "\n",
    "            # -------- 改成往 list 里 append --------\n",
    "            data[\"key\"].append(key)\n",
    "            data[\"adj_matrix\"].append(adj_matrix)\n",
    "            data[\"features\"].append(features)\n",
    "            data[\"flops\"].append(flops)\n",
    "            data[\"params\"].append(params)\n",
    "            data[\"accuracy\"].append(accuracy)\n",
    "            data[\"latency\"].append(latency)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Warning] Skipped item {i} due to: {e}\")\n",
    "\n",
    "    for key in data:\n",
    "        data[key] = np.array(data[key])\n",
    "        print(f\"{key} shape: {data[key].shape}\")\n",
    "\n",
    "    return data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
